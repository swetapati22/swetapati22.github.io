<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
Research conducted at George Mason University (GMU) - Natural Language Processing Lab
Research Advisor: Dr. Ziyu Yao
Authors: Saurabh Srivastava★, Sweta Pati★, Dr. Ziyu Yao  [★ Equal contribution]
Conference Submission: ACL 2025
---
</code></pre></div></div> <hr> <h3 id="research-paper--resources"><strong>Research Paper &amp; Resources:</strong></h3> <ul> <li> <strong>ACL 2025 (Findings)</strong>: <a href="https://aclanthology.org/2025.findings-acl.677/" target="_blank" rel="external nofollow noopener">Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines</a> </li> <li> <strong>Research Institution</strong>: George Mason University (GMU), NLP Lab</li> <li> <strong>Research Advisor:</strong> Dr. Ziyu Yao</li> <li> <strong>Focus Area</strong>: <strong>Event Extraction</strong>, <strong>Instruction-Tuned LLMs</strong>, <strong>Annotation Guideline Generation</strong>, <strong>LLM FineTuning</strong> </li> <li> <strong>Code Repository</strong>: <a href="https://github.com/Ziyu-Yao-NLP-Lab/PyCode-TextEE" target="_blank" rel="external nofollow noopener">PyCode-TextEE</a> </li> </ul> <hr> <blockquote> <p>⚡ <strong>TL;DR</strong>: We instruction-tune LLMs for event extraction using Python code prompts and machine-generated annotation guidelines — achieving strong generalization across domains, schemas, and model sizes with minimal supervision.</p> </blockquote> <hr> <h3 id="tech-stack--tools"><strong>Tech Stack &amp; Tools:</strong></h3> <ul> <li> <strong>NLP and Machine Learning</strong>: LLaMA-3.1-8B, LLaMA-3.2-1B, and Qwen2.5-Coder-1.5B, Hugging Face Transformers, PyTorch, LoRA Fine-Tuning, Unsloth, Quantization</li> <li> <strong>Data Processing</strong>: JSON, Python (Pandas, Numpy)</li> <li> <strong>GPU Resources</strong>: HPC clusters, CUDA-enabled GPUs</li> <li> <strong>Evaluation Metrics</strong>: Precision, Recall, F1-score</li> </ul> <hr> <h3 id="context--motivation"><strong>Context &amp; Motivation:</strong></h3> <p>Event Extraction (EE) is a fundamental task in information extraction but remains challenging due to:</p> <ul> <li>Complex schemas</li> <li>Domain shifts</li> <li>Low-resource settings</li> </ul> <p>Our work investigates how <strong>instruction-tuned LLMs</strong> can benefit from <strong>structured guidance</strong> in the form of event schemas and machine-generated annotation guidelines.</p> <hr> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/EE_PyCode_example-480.webp 480w,/assets/img/EE_PyCode_example-800.webp 800w,/assets/img/EE_PyCode_example-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/EE_PyCode_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Overview of code prompt with annotation guidelines" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="approach"><strong>Approach:</strong></h3> <p>This research explores the <strong>enhancement of event extraction (EE) tasks</strong> by leveraging <strong>instruction-tuned large language models (LLMs)</strong>. Traditional event extraction models struggle with <strong>limited training data, ambiguous event definitions, and scalability</strong>. To address these challenges, our approach:</p> <ul> <li> <strong>Synthesized annotation guidelines using GPT-4o</strong> for: <ul> <li><strong>500+ event types</strong></li> <li><strong>4000+ argument structures</strong></li> <li>Examples include <strong>positive, negative</strong>, and <strong>sibling event types</strong> </li> <li>Removes reliance on manually written guidelines</li> </ul> </li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/EE_annotation_guidelines-480.webp 480w,/assets/img/EE_annotation_guidelines-800.webp 800w,/assets/img/EE_annotation_guidelines-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/EE_annotation_guidelines.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Example for annotation guidelines" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Natural Language to Code-Based Prompt Conversion</strong>: <ul> <li>We provide a modular conversion script that transforms <strong>natural language event extraction data (in TextEE format)</strong> into structured <strong>Python <code class="language-plaintext highlighter-rouge">@dataclass</code> prompts</strong> with embedded <strong>annotation guidelines</strong>.</li> <li>Instruction prompts use: <ul> <li>We <strong>instruction-tune LLMs</strong> with structured prompts that represent events as Python <code class="language-plaintext highlighter-rouge">@dataclass</code> definitions.</li> <li> <strong>Annotation guidelines</strong> are integrated directly into prompt <strong>docstrings</strong>, defining triggers and arguments in natural language.</li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/EE_code_prompt_example-480.webp 480w,/assets/img/EE_code_prompt_example-800.webp 800w,/assets/img/EE_code_prompt_example-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/EE_code_prompt_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Example for a code prompt with annotation guidelines" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p><strong>Fine-tuning models</strong>:<br> <strong>LLaMA-3.1 8B, LLaMA-3.2-1B, and Qwen2.5-Coder-1.5B</strong> using <strong>LoRA</strong> and <strong>structured regularization</strong></p> </li> <li> <p><strong>Designed custom evaluation framework</strong> for EE tasks designed around structured code-like annotations</p> </li> </ul> <p>This work aims to improve <strong>F1-score performance</strong> on <strong>event extraction tasks</strong> and <strong>enhance the reliability</strong> of LLM-generated event predictions.</p> <hr> <h3 id="experiments"><strong>Experiments:</strong></h3> <p>We conducted evaluations on:</p> <ul> <li> <strong>Datasets:</strong> ACE05 and RichERE</li> <li> <strong>Settings:</strong> Full-resource and low-resource (2k samples)</li> <li> <strong>Variants:</strong> With and without contrastive training (negative sampling)</li> </ul> <hr> <h3 id="key-results"><strong>Key Results:</strong></h3> <h4 id="ace05-machine-generated-guidelines-no-negative-sampling">ACE05 (machine-generated guidelines, <em>no negative sampling</em>)</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Trigger Classification (TC): +10% improvement  
- Argument Classification (AC): +5% improvement  
</code></pre></div></div> <h4 id="richere-machine-generated-guidelines-with-negative-sampling">RichERE (machine-generated guidelines, <em>with negative sampling</em>)</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Trigger Classification (TC): +30% improvement  
- Argument Classification (AC): +25% improvement  
</code></pre></div></div> <hr> <h3 id="major-contributions"><strong>Major Contributions</strong></h3> <h4 id="1-annotation-guideline-optimization"><strong>1. Annotation Guideline Optimization</strong></h4> <ul> <li>Developed a structured framework for creating <strong>high-quality event extraction annotation guidelines</strong>.</li> <li>Synthesized event schema covering <strong>500+ event types</strong> and <strong>4000+ argument structures</strong> using <strong>GPT-4o</strong>.</li> </ul> <h4 id="2-natural-language-to-code-based-prompt-conversion"><strong>2. Natural Language to Code-Based Prompt Conversion:</strong></h4> <ul> <li>Developed a script that translates natural language event definitions into Python @dataclass-style code prompts</li> <li>These prompts include annotation guidelines as docstrings, making them executable and interpretable by LLMs</li> <li>Converts event triggers and arguments into <strong>Python-style classes</strong> </li> <li>Embeds <strong>natural language instructions</strong> as <strong>docstrings</strong> for each event type and role</li> <li>Supports <strong>automatic annotation guideline integration</strong> using synthesized or provided files</li> <li> <p>Generates <strong>LLM-compatible structured prompts</strong> for instruction tuning</p> </li> <li>This script is a core part of the pipeline that bridges textual datasets and code-style prompt learning for LLMs like LLaMA-3 and Qwen.</li> </ul> <h4 id="3-llm-fine-tuning--optimization"><strong>3. LLM Fine-Tuning &amp; Optimization</strong></h4> <ul> <li>Implemented <strong>Low-Rank Adaptation (LoRA)</strong> for <strong>fine-tuning LLaMA-3.1 8B, LLaMA-3.2-1B, and Qwen2.5-Coder-1.5B</strong> on event extraction tasks using <strong>unsloth</strong> library.</li> <li>Improved <strong>model efficiency</strong> using <strong>structured regularization</strong> techniques.</li> </ul> <h4 id="4-cost-effective--low-latency-inference-pipelines"><strong>4. Cost-Effective &amp; Low-Latency Inference Pipelines</strong></h4> <ul> <li>Designed a <strong>scalable NLP pipeline</strong> using <strong>GPU-optimized environments</strong>.</li> <li>Designed a <strong>custom evaluation framework</strong> guided by annotations for each event type for event extraction tasks as code representations.</li> </ul> <hr> <h3 id="performance-highlights"><strong>Performance Highlights:</strong></h3> <ul> <li> <strong>Machine-generated guidelines</strong> consistently outperformed <strong>human-written ones</strong>.</li> <li>The approach <strong>generalized well across domains, schema complexities, and model architectures</strong>.</li> <li>In <strong>low-data settings (2k)</strong>, models with guidelines <strong>matched or exceeded full-data baselines</strong>.</li> <li> <strong>Frequent and moderately rare event types</strong> showed notable improvement, <strong>extremely rare types remain a challenge</strong>.</li> <li>While guidelines help, we found that <strong>machine-generated guidelines consistently outperform human-written ones</strong>, and their <strong>effectiveness may or may not complement negative sampling strategies</strong>.</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/EE_evaluation_results-480.webp 480w,/assets/img/EE_evaluation_results-800.webp 800w,/assets/img/EE_evaluation_results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/EE_evaluation_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Evaluation Results across different settings" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h3 id="future-work--applications"><strong>Future Work &amp; Applications</strong></h3> <p><strong>Expanding dataset coverage</strong> to include <strong>multi-domain event extraction</strong> tasks<br> <strong>Exploring multimodal event extraction</strong> by integrating <strong>text, images, and video content</strong></p> <hr> <p><em>For collaboration or inquiries, feel free to reach out via <a href="https://www.linkedin.com/in/sweta-pati/" rel="external nofollow noopener" target="_blank">LinkedIn</a> or <a href="mailto:spati@gmu.edu">Email</a>.</em></p> </body></html>